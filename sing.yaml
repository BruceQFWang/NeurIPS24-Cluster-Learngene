description: OT-learngene
target:
  service: sing
  name: msrresrchvc
  workspace_name: msrresrchws
  # name: msroctovc
  # workspace_name: msroctows

environment:
  image: ninja0/pytorch:22.05-py3 #image: zeliu98/pytorch:pytorch1.8.1-py38-cuda11.3-cudnn820-openmpi-mmcv-apex-timm   #amlt-sing/pytorch-1.7.0-cuda11.0-cudnn8-devel
  registry: docker.io # any public registry can be specified here
  setup:
    - pip install --upgrade numpy --user
    - pip install tensorboardX --user
    - pip install timm --user
    - pip install tensorboard --user
    - pip install easydict --user
    - pip install scipy --user
    - pip install pathlib --user
    - pip install pillow --user
    - pip install torchnet --user
    - pip install functools --user
    - pip install regex --user
    - pip install ftfy --user
    - pip install gzip --user
    - pip install ml-collections --user
    - pip install collections --user
    - pip install thop --user
    

storage:
  # storage account and container where the ImageNet tar balls are contained
  data:
    storage_account_name: itpeus4data
    container_name: v-miazhang
    mount_dir: /dataset
  output:
    storage_account_name: itpeus4data
    container_name: v-miazhang
    mount_dir: /output

code:
  local_dir: $CONFIG_DIR


jobs:
- name: deit_tiny
  sku: G8-V100
  priority: High
  process_count_per_node: 1
  mpi: True
  command:
  - echo " Begin my runs "
  - export github_token=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; git clone https://$${github_token}@github.com/BruceQFWang/Optimal-transport-Learngene.git -b master_v103; cd Optimal-transport-Learngene;
  - bash deit_tiny.sh
  # - sleep infinity
  submit_args:
    env:
      AMLT_NO_TENSORBOARD_PATCHING: 1
      NCCL_IB_DISABLE: 0
      NCCL_DEBUG: INFO
      TORCH_DISTRIBUTED_DEBUG: DETAIL
      NCCL_IB_TIMEOUT: 22
      MKL_THREADING_LAYER: GNU
      AZUREML_CMK8S_SKIP_ENVPREPARE: True
# - name: deit_tiny_ablation_4
#   sku: G8-V100
#   priority: High
#   process_count_per_node: 1
#   mpi: True
#   command:
#   - echo " Begin my runs "
#   - export github_token=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; git clone https://$${github_token}@github.com/BruceQFWang/Optimal-transport-Learngene.git -b master_v103; cd Optimal-transport-Learngene;
#   - bash deit_tiny_ablation_4.sh
#   # - sleep infinity
#   submit_args:
#     env:
#       AMLT_NO_TENSORBOARD_PATCHING: 1
#       NCCL_IB_DISABLE: 0
#       NCCL_DEBUG: INFO
#       TORCH_DISTRIBUTED_DEBUG: DETAIL
#       NCCL_IB_TIMEOUT: 22
#       MKL_THREADING_LAYER: GNU
#       AZUREML_CMK8S_SKIP_ENVPREPARE: True
# - name: deit_tiny_ablation_3
#   sku: G8-V100
#   priority: High
#   process_count_per_node: 1
#   mpi: True
#   command:
#   - echo " Begin my runs "
#   - export github_token=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; git clone https://$${github_token}@github.com/BruceQFWang/Optimal-transport-Learngene.git -b master_v103; cd Optimal-transport-Learngene;
#   - bash deit_tiny_ablation_3.sh
#   # - sleep infinity
#   submit_args:
#     env:
#       AMLT_NO_TENSORBOARD_PATCHING: 1
#       NCCL_IB_DISABLE: 0
#       NCCL_DEBUG: INFO
#       TORCH_DISTRIBUTED_DEBUG: DETAIL
#       NCCL_IB_TIMEOUT: 22
#       MKL_THREADING_LAYER: GNU
#       AZUREML_CMK8S_SKIP_ENVPREPARE: True
# - name: deit_tiny_ablation_2
#   sku: G8-V100
#   priority: High
#   process_count_per_node: 1
#   mpi: True
#   command:
#   - echo " Begin my runs "
#   - export github_token=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; git clone https://$${github_token}@github.com/BruceQFWang/Optimal-transport-Learngene.git -b master_v103; cd Optimal-transport-Learngene;
#   - bash deit_tiny_ablation_2.sh
#   # - sleep infinity
#   submit_args:
#     env:
#       AMLT_NO_TENSORBOARD_PATCHING: 1
#       NCCL_IB_DISABLE: 0
#       NCCL_DEBUG: INFO
#       TORCH_DISTRIBUTED_DEBUG: DETAIL
#       NCCL_IB_TIMEOUT: 22
#       MKL_THREADING_LAYER: GNU
#       AZUREML_CMK8S_SKIP_ENVPREPARE: True
# - name: deit_tiny_ablation_1
#   sku: G8-V100
#   priority: High
#   process_count_per_node: 1
#   mpi: True
#   command:
#   - echo " Begin my runs "
#   - export github_token=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; git clone https://$${github_token}@github.com/BruceQFWang/Optimal-transport-Learngene.git -b master_v103; cd Optimal-transport-Learngene;
#   - bash deit_tiny_ablation_1.sh
#   # - sleep infinity
#   submit_args:
#     env:
#       AMLT_NO_TENSORBOARD_PATCHING: 1
#       NCCL_IB_DISABLE: 0
#       NCCL_DEBUG: INFO
#       TORCH_DISTRIBUTED_DEBUG: DETAIL
#       NCCL_IB_TIMEOUT: 22
#       MKL_THREADING_LAYER: GNU
#       AZUREML_CMK8S_SKIP_ENVPREPARE: True

# - name: deit_small
#   sku: G8-V100
#   priority: High
#   process_count_per_node: 1
#   mpi: True
#   command:
#   - echo " Begin my runs "
#   - export github_token=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; git clone https://$${github_token}@github.com/BruceQFWang/Optimal-transport-Learngene.git -b master_v103; cd Optimal-transport-Learngene;
#   - bash deit_small.sh
#   # - sleep infinity
#   submit_args:
#     env:
#       AMLT_NO_TENSORBOARD_PATCHING: 1
#       NCCL_IB_DISABLE: 0
#       NCCL_DEBUG: INFO
#       TORCH_DISTRIBUTED_DEBUG: DETAIL
#       NCCL_IB_TIMEOUT: 22
#       MKL_THREADING_LAYER: GNU
#       AZUREML_CMK8S_SKIP_ENVPREPARE: True 

# - name: deit_small_depth_1
#   sku: G8-V100
#   priority: High
#   process_count_per_node: 1
#   mpi: True
#   command:
#   - echo " Begin my runs "
#   - export github_token=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; git clone https://$${github_token}@github.com/BruceQFWang/Optimal-transport-Learngene.git -b master_v103; cd Optimal-transport-Learngene;
#   - bash deit_small_depth_1.sh
#   # - sleep infinity
#   submit_args:
#     env:
#       AMLT_NO_TENSORBOARD_PATCHING: 1
#       NCCL_IB_DISABLE: 0
#       NCCL_DEBUG: INFO
#       TORCH_DISTRIBUTED_DEBUG: DETAIL
#       NCCL_IB_TIMEOUT: 22
#       MKL_THREADING_LAYER: GNU
#       AZUREML_CMK8S_SKIP_ENVPREPARE: True 

# - name: deit_small_depth_2
#   sku: G8-V100
#   priority: High
#   process_count_per_node: 1
#   mpi: True
#   command:
#   - echo " Begin my runs "
#   - export github_token=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; git clone https://$${github_token}@github.com/BruceQFWang/Optimal-transport-Learngene.git -b master_v103; cd Optimal-transport-Learngene;
#   - bash deit_small_depth_2.sh
#   # - sleep infinity
#   submit_args:
#     env:
#       AMLT_NO_TENSORBOARD_PATCHING: 1
#       NCCL_IB_DISABLE: 0
#       NCCL_DEBUG: INFO
#       TORCH_DISTRIBUTED_DEBUG: DETAIL
#       NCCL_IB_TIMEOUT: 22
#       MKL_THREADING_LAYER: GNU
#       AZUREML_CMK8S_SKIP_ENVPREPARE: True 

# - name: deit_small_depth_3
#   sku: G8-V100
#   priority: High
#   process_count_per_node: 1
#   mpi: True
#   command:
#   - echo " Begin my runs "
#   - export github_token=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; git clone https://$${github_token}@github.com/BruceQFWang/Optimal-transport-Learngene.git -b master_v103; cd Optimal-transport-Learngene;
#   - bash deit_small_depth_3.sh
#   # - sleep infinity
#   submit_args:
#     env:
#       AMLT_NO_TENSORBOARD_PATCHING: 1
#       NCCL_IB_DISABLE: 0
#       NCCL_DEBUG: INFO
#       TORCH_DISTRIBUTED_DEBUG: DETAIL
#       NCCL_IB_TIMEOUT: 22
#       MKL_THREADING_LAYER: GNU
#       AZUREML_CMK8S_SKIP_ENVPREPARE: True 

# - name: deit_small_baseline
#   sku: G8-V100
#   priority: High
#   process_count_per_node: 1
#   mpi: True
#   command:
#   - echo " Begin my runs "
#   - export github_token=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; git clone https://$${github_token}@github.com/BruceQFWang/Optimal-transport-Learngene.git -b master_v103; cd Optimal-transport-Learngene;
#   - bash deit_small_baseline.sh
#   # - sleep infinity
#   submit_args:
#     env:
#       AMLT_NO_TENSORBOARD_PATCHING: 1
#       NCCL_IB_DISABLE: 0
#       NCCL_DEBUG: INFO
#       TORCH_DISTRIBUTED_DEBUG: DETAIL
#       NCCL_IB_TIMEOUT: 22
#       MKL_THREADING_LAYER: GNU
#       AZUREML_CMK8S_SKIP_ENVPREPARE: True 

# - name: deit_small_ablation_1
#   sku: G8-V100
#   priority: High
#   process_count_per_node: 1
#   mpi: True
#   command:
#   - echo " Begin my runs "
#   - export github_token=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; git clone https://$${github_token}@github.com/BruceQFWang/Optimal-transport-Learngene.git -b master_v103; cd Optimal-transport-Learngene;
#   - bash deit_small_ablation_1.sh
#   # - sleep infinity
#   submit_args:
#     env:
#       AMLT_NO_TENSORBOARD_PATCHING: 1
#       NCCL_IB_DISABLE: 0
#       NCCL_DEBUG: INFO
#       TORCH_DISTRIBUTED_DEBUG: DETAIL
#       NCCL_IB_TIMEOUT: 22
#       MKL_THREADING_LAYER: GNU
#       AZUREML_CMK8S_SKIP_ENVPREPARE: True 

# - name: deit_small_ablation_2
#   sku: G8-V100
#   priority: High
#   process_count_per_node: 1
#   mpi: True
#   command:
#   - echo " Begin my runs "
#   - export github_token=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; git clone https://$${github_token}@github.com/BruceQFWang/Optimal-transport-Learngene.git -b master_v103; cd Optimal-transport-Learngene;
#   - bash deit_small_ablation_2.sh
#   # - sleep infinity
#   submit_args:
#     env:
#       AMLT_NO_TENSORBOARD_PATCHING: 1
#       NCCL_IB_DISABLE: 0
#       NCCL_DEBUG: INFO
#       TORCH_DISTRIBUTED_DEBUG: DETAIL
#       NCCL_IB_TIMEOUT: 22
#       MKL_THREADING_LAYER: GNU
#       AZUREML_CMK8S_SKIP_ENVPREPARE: True 

# - name: deit_small_ablation_3
#   sku: G8-V100
#   priority: High
#   process_count_per_node: 1
#   mpi: True
#   command:
#   - echo " Begin my runs "
#   - export github_token=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; git clone https://$${github_token}@github.com/BruceQFWang/Optimal-transport-Learngene.git -b master_v103; cd Optimal-transport-Learngene;
#   - bash deit_small_ablation_3.sh
#   # - sleep infinity
#   submit_args:
#     env:
#       AMLT_NO_TENSORBOARD_PATCHING: 1
#       NCCL_IB_DISABLE: 0
#       NCCL_DEBUG: INFO
#       TORCH_DISTRIBUTED_DEBUG: DETAIL
#       NCCL_IB_TIMEOUT: 22
#       MKL_THREADING_LAYER: GNU
#       AZUREML_CMK8S_SKIP_ENVPREPARE: True 

# - name: deit_small_ablation_4
#   sku: G8-V100
#   priority: High
#   process_count_per_node: 1
#   mpi: True
#   command:
#   - echo " Begin my runs "
#   - export github_token=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; git clone https://$${github_token}@github.com/BruceQFWang/Optimal-transport-Learngene.git -b master_v103; cd Optimal-transport-Learngene;
#   - bash deit_small_ablation_4.sh
#   # - sleep infinity
#   submit_args:
#     env:
#       AMLT_NO_TENSORBOARD_PATCHING: 1
#       NCCL_IB_DISABLE: 0
#       NCCL_DEBUG: INFO
#       TORCH_DISTRIBUTED_DEBUG: DETAIL
#       NCCL_IB_TIMEOUT: 22
#       MKL_THREADING_LAYER: GNU
#       AZUREML_CMK8S_SKIP_ENVPREPARE: True 